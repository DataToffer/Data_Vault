<!DOCTYPE html>
<!-- saved from url=(0058)https://quickstarts.snowflake.com/guide/vhol_data_vault/#0 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  
  <title>Building a Real-Time Data Vault in Snowflake</title>
  <link rel="stylesheet" href="./Building a Real-Time Data Vault in Snowflake_files/css">
  <link rel="stylesheet" href="./Building a Real-Time Data Vault in Snowflake_files/icon">
  <link rel="stylesheet" href="./Building a Real-Time Data Vault in Snowflake_files/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
<script type="text/javascript" async="" src="./Building a Real-Time Data Vault in Snowflake_files/heap-2025084205.js.descarga"></script><script src="./Building a Real-Time Data Vault in Snowflake_files/analytics.js.descarga"></script></head>
<body>
  <google-codelab-analytics gaid="UA-41491190-9" environment="web" category="null"></google-codelab-analytics>
  <google-codelab codelab-gaid="" id="vhol_data_vault" environment="web" feedback-link="https://github.com/Snowflake-Labs/sfguides/issues" selected="0" google-codelab-ready="" codelab-title="Building a Real-Time Data Vault in Snowflake" anayltics-ready="anayltics-ready"><div id="drawer"><div class="steps"><ol><li completed="" selected=""><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#0"><span class="step"><span>Overview</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#1"><span class="step"><span>Reference Architecture</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#2"><span class="step"><span>Environment setup</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#3"><span class="step"><span>Data Pipelines: Design</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#4"><span class="step"><span>Sample data &amp; staging area</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#5"><span class="step"><span>Build: Raw Data Vault</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#6"><span class="step"><span>Views for Agile Reporting</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#7"><span class="step"><span>Build: Business Data Vault</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#8"><span class="step"><span>Build: Information Delivery</span></span></a></li><li><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#9"><span class="step"><span>Conclusion</span></span></a></li></ol></div><div class="metadata"><a target="_blank" href="https://github.com/Snowflake-Labs/sfguides/issues"><i class="material-icons">bug_report</i> Report a mistake</a></div></div><div id="codelab-title"><div id="codelab-nav-buttons"><a href="https://quickstarts.snowflake.com/" id="arrow-back"><i class="material-icons">close</i></a><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#" id="menu"><i class="material-icons">menu</i></a></div><h1 class="title">Building a Real-Time Data Vault in Snowflake</h1><div id="codelab-time-container"><div id="time-remaining" title="Time remaining"><i class="material-icons">access_time</i>69 mins remaining</div></div><devsite-user></devsite-user></div><div id="main"><div id="steps"><google-codelab-step label="Overview" duration="3" step="1" style="transform: translate3d(0px, 0px, 0px);" selected=""><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">1. Overview</h2>
        <p>In this day and age, with the ever-increasing availability and volume of data from many types of sources such as IoT, mobile devices, and weblogs, there is a growing need, and yes, demand, to go from batch load processes to streaming or "real-time" (RT) loading of data. Businesses are changing at an alarming rate and are becoming more competitive all the time. Those that can harness the value of their data faster to drive better business outcomes will be the ones to prevail.</p>
<p>One of the benefits of using the Data Vault 2.0 architecture is that it was designed from inception not only to accept data loaded using traditional batch mode (which was the prevailing mode in the early 2000s when <a href="https://datavaultalliance.com/certification/standards-board/" target="_blank">Dan Linstedt</a> introduced Data Vault) but also to easily accept data loading in real or near-realtime (NRT). In the early 2000s, that was a nice-to-have aspect of the approach and meant the methodology was effectively future-proofed from that perspective. Still, few database systems had the capacity to support that kind of requirement. Today, RT or at least NRT loading is almost becoming a mandatory requirement for modern data platforms. Granted, not all loads or use cases need to be NRT, but most forward-thinking organizations need to onboard data for analytics in an NRT manner.</p>
<p>Those who have been using the Data Vault approach don't need to change much other than figure out how to engineer their data pipeline to serve up data to the Data Vault in NRT. The data models don't need to change; the reporting views don't need to change; even the loading patterns don't need to change. (NB: For those that aren't using Data Vault already, if they have real-time loading requirements, this architecture and method might be worth considering.)</p>
<h2 is-upgraded="">Data Vault on Snowflake</h2>
<p>There have been numerous <a href="https://www.snowflake.com/blog/tips-for-optimizing-the-data-vault-architecture-on-snowflake/" target="_blank">blog posts</a>, user groups, and webinars over the last few years, discussing the best practices and customer success stories around implementing Data Vaults on Snowflake.  So the question now is how do you build a Data Vault on Snowflake that has real-time or near real-time data streaming into it.</p>
<p>Luckily, streaming data is one of the <a href="https://www.snowflake.com/cloud-data-platform/" target="_blank">use-cases</a> that Snowflake was built to support, so we have many features to help us achieve this goal. <strong>This guide is an extended version of the </strong><a href="https://datavaultalliance.com/news/building-a-real-time-data-vault-in-snowflake/" target="_blank"><strong>article</strong></a><strong> posted on Data Vault Alliance website, now including practical steps to build an example of real-time Data Vault feed on Snowflake. Join us on simple-to-follow steps to see it in action.</strong></p>
<h2 is-upgraded="">Prerequisites</h2>
<ul>
<li>A Snowflake account. Existing or if you are not(yet) a Snowflake user, you can always get a <a href="https://trial.snowflake.com/" target="_blank">trial</a> account</li>
<li>Familiarity with Snowflake and Snowflake objects</li>
<li>Understanding of Data Vault concepts and modelling techniques</li>
</ul>
<h2 class="checklist" is-upgraded="">What You'll Learn</h2>
<ul class="checklist">
<li>how to use Data Vault on Snowflake</li>
<li>how to build basic objects and write ELT code for it</li>
<li>how to leverage <a href="https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro.html" target="_blank">Snowpipe</a> and <a href="https://docs.snowflake.com/en/user-guide/data-pipelines.html" target="_blank">Continous Data Pipelines</a> to automate data processing</li>
<li>how to apply data virtualization to accellerate data access</li>
</ul>
<h2 is-upgraded="">What You'll Build</h2>
<ul>
<li>a Data Vault environment on Snowflake, based on sample dataset</li>
<li>data pipelines, leveraging streams, tasks and Snowpipe</li>
</ul>


      </div></div></google-codelab-step><google-codelab-step label="Reference Architecture" duration="5" step="2"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">2. Reference Architecture</h2>
        <p>Let's start with the overall architecture to put everything in context.</p>
<p class="image-container"><img alt="dbt_project.yml" src="./Building a Real-Time Data Vault in Snowflake_files/251b7f5870f72299.png"></p>
<p>On the very left of figure above we have a list of <strong>data providers</strong> that typically include a mix of existing operational databases, old data warehouses, files, lakes as well as 3rd party apps. There is now also the  possibility to leverage Snowflake Data Sharing/Marketplace as a way to tap into new 3rd party data assets to augment your data set.</p>
<p>On the very right we have our ultimate <strong>data consumers</strong>: business users, data scientists, IT systems or even other companies you decided to exchange your data with.</p>
<p>Architecturally, we will split the data lifecycle into following layers:</p>
<ul>
<li><strong>Data Acquisition:</strong> extracting data from source systems and making it accessible for Snowflake.</li>
<li><strong>Loading &amp; Staging:</strong> moving the source data into Snowflake. For this Snowflake has multiple options, including batch load, external tables and Snowpipe(our managed service for onboarding streaming data). Snowflake allows you to load and store structured and semi-structured in the original format whilst automatically optimizing the physical structure for efficient query access. The data is immutable and should be stored as it was received from source with no changes to the content. From a Data Vault perspective, functionally, this layer is also responsible for adding technical metadata (record source,, load date timestamp, etc.) as well as calculating business keys.</li>
<li><strong>Raw Data Vault:</strong> a data vault model with no soft business rules or transformations applied (only hard rules are allowed) loading all records received from source.</li>
<li><strong>Business Data Vault:</strong> data vault objects with soft business rules applied. The raw data vault data is getting augmented by the intelligence of the system. It is not a copy of the raw data vault, but rather a sparse addition with perhaps calculated satellites,  mastered records,or maybe even commonly used aggregations. This could also optionally include PIT and Bridge tables helping to simplify access to bi-temporal view of the data. From a Snowflake perspective, raw and business data vaults could be separated by object naming convention or represented as different schemas or even different databases.</li>
<li><strong>Information Delivery:</strong> a layer of consumer-oriented models. This could be implemented as a set (or multiple sets) of views. It is common to see the use of dimensional models (star/snowflake) or denormalized flat tables (for example for data science or sharing) but it could be any other modeling stye (e.g., unified star schema, supernova, key-value, document object mode, etc.) that fits best for your data consumer. Snowflake's scalability will support the required speed of access at any point of this data lifecycle. You should consider Business Vault and Information Delivery objects materialization as optional. This specific topic (virtualization) is going to be covered later in this article.</li>
</ul>


      </div></div></google-codelab-step><google-codelab-step label="Environment setup" duration="5" step="3"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">3. Environment setup</h2>
        <ol type="1">
<li>Login to your Snowflake trial account.<br><img alt="Snowflake Log In Screen" src="./Building a Real-Time Data Vault in Snowflake_files/65dcd59b3b8b1a.png"></li>
<li>First page you are going to see would likely be <a href="https://docs.snowflake.com/en/user-guide/ui-using.html" target="_blank">Snowflake Classic UI</a>: <img alt="Snowflake Classic UI" src="./Building a Real-Time Data Vault in Snowflake_files/84b31301b206662f.png"><br> To keep things interesting, for the purpose of this lab let's use <a href="https://docs.snowflake.com/en/user-guide/ui-web.html" target="_blank">Snowflake New Web Interface</a> also known as Snowsight. However, you absolutely can continue using Classic UI as all steps in this guide are expressed in SQL and will work regardless what interface is used. To switch into Snowsight, let's click the <strong>Preview</strong> button in the top-right corner:</li>
</ol>
<p class="image-container"><img alt="Snowflake New UI" src="./Building a Real-Time Data Vault in Snowflake_files/994f41b1b2825492.png"></p>
<p>Click Sign in to continue. You will need to use the same user and password that you used to login to your Snowflake account the first time.</p>
<p class="image-container"><img alt="Sign In" src="./Building a Real-Time Data Vault in Snowflake_files/74156d415612bbc0.png"></p>
<p>You're now in the new UI - Snowsight. It's pretty cool - with charting, dashboards, autocompletion and new capabilities that our engineering team will continue to add on weekly. Now, let's click on Worksheets...</p>
<ol type="1" start="3">
<li>Let's click on the worksheets -&gt; <strong>+ Worksheet</strong><img alt="Sign In" src="./Building a Real-Time Data Vault in Snowflake_files/d039748c64e62359.png"></li>
</ol>
<p>And without going into too much details, this is a fairly intuitive SQL workbench. It has a section for code we are going to be copy-pasting, object tree on the left, the ‘run' button and of course the result panel at the bottom with the simple charting functionality. <img alt="Worksheet" src="./Building a Real-Time Data Vault in Snowflake_files/abc9dbce37df0786.png"></p>
<ol type="1" start="4">
<li>We are going to start by setting up basics for the lab environment. Creating a clean database and logically dividing it into four different schemas, representing each functional area mentioned in the reference architecture.</li>
</ol>
<p>To keep things simple, we are going to use the ACCOUNTADMIN role (thou, of course in the real life examples you would employ a proper RBAC model). We also going to create two <a href="https://docs.snowflake.com/en/user-guide/warehouses.html" target="_blank">Snowflake virtual warehouses</a> to manage compute - one for generic use during the course of this lab and the other one (dv_rdv_wh) that is going to be used by our data pipelines. You might notice that the code for two more virtual warehouses (dv_bdv_wh, dv_id_wh) is commented - again, this is just to keep things simple for the guide but we wanted to illustrate the fact you can have as many of virtual warehouses of any size and configuration as you needed. For example having separate ones to deal with different layers in our Data Vault architecture.</p>
<pre><code language="language-sql" class="language-sql"><span class="pun">--------------------------------------------------------------------</span><span class="pln">
</span><span class="pun">--</span><span class="pln"> setting up the environment
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">

USE ROLE accountadmin</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE DATABASE dv_lab</span><span class="pun">;</span><span class="pln">

USE DATABASE dv_lab</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE WAREHOUSE dv_lab_wh WITH WAREHOUSE_SIZE </span><span class="pun">=</span><span class="pln"> </span><span class="str">'XSMALL'</span><span class="pln"> MIN_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> MAX_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> AUTO_SUSPEND </span><span class="pun">=</span><span class="pln"> </span><span class="lit">60</span><span class="pln"> COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'Generic WH'</span><span class="pun">;</span><span class="pln">
CREATE OR REPLACE WAREHOUSE dv_rdv_wh WITH WAREHOUSE_SIZE </span><span class="pun">=</span><span class="pln"> </span><span class="str">'XSMALL'</span><span class="pln"> MIN_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> MAX_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> AUTO_SUSPEND </span><span class="pun">=</span><span class="pln"> </span><span class="lit">60</span><span class="pln"> COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'WH for Raw Data Vault object pipelines'</span><span class="pun">;</span><span class="pln">
</span><span class="pun">--</span><span class="pln">CREATE OR REPLACE WAREHOUSE dv_bdv_wh WITH WAREHOUSE_SIZE </span><span class="pun">=</span><span class="pln"> </span><span class="str">'XSMALL'</span><span class="pln"> MIN_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> MAX_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> AUTO_SUSPEND </span><span class="pun">=</span><span class="pln"> </span><span class="lit">60</span><span class="pln"> COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'WH for Business Data Vault object pipelines'</span><span class="pun">;</span><span class="pln">
</span><span class="pun">--</span><span class="pln">CREATE OR REPLACE WAREHOUSE dv_id_wh  WITH WAREHOUSE_SIZE </span><span class="pun">=</span><span class="pln"> </span><span class="str">'XSMALL'</span><span class="pln"> MIN_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> MAX_CLUSTER_COUNT </span><span class="pun">=</span><span class="pln"> </span><span class="lit">1</span><span class="pln"> AUTO_SUSPEND </span><span class="pun">=</span><span class="pln"> </span><span class="lit">60</span><span class="pln"> COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'WH for information delivery object pipelines'</span><span class="pun">;</span><span class="pln">

USE WAREHOUSE dv_lab_wh</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE SCHEMA l00_stg COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'Schema for Staging Area objects'</span><span class="pun">;</span><span class="pln">
CREATE OR REPLACE SCHEMA l10_rdv COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'Schema for Raw Data Vault objects'</span><span class="pun">;</span><span class="pln">
CREATE OR REPLACE SCHEMA l20_bdv COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'Schema for Business Data Vault objects'</span><span class="pun">;</span><span class="pln">
CREATE OR REPLACE SCHEMA l30_id  COMMENT </span><span class="pun">=</span><span class="pln"> </span><span class="str">'Schema for Information Delivery objects'</span><span class="pun">;</span></code></pre>


      </div></div></google-codelab-step><google-codelab-step label="Data Pipelines: Design" duration="10" step="4"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">4. Data Pipelines: Design</h2>
        <p class="image-container"><img alt="dbt_project.yml" src="./Building a Real-Time Data Vault in Snowflake_files/105e5893bca6e7ac.png"></p>
<p>Snowflake supports multiple options for engineering data pipelines. In this post we are going to show one of the most efficient ways to implement incremental NRT integration leveraging Snowflake <a href="https://docs.snowflake.com/en/user-guide/data-pipelines.html" target="_blank">Continuous Data Pipelines</a>.  Let's take a look at the architecture diagram above to understand how it works.</p>
<p>Snowflake has a special <a href="https://docs.snowflake.com/en/user-guide/streams.html" target="_blank">stream</a> object that tracks all data changes on a table (inserts, updates, and deletes). This process is 100% automatic and unlike traditional databases will never impact the speed of data loading. The change log from a stream is automatically ‘consumed' once there is a successfully completed DML operation using the stream object as a source.</p>
<p>So, loading new data into a staging table, would immediately be reflected in a stream showing the <a href="https://docs.snowflake.com/en/user-guide/streams.html#data-flow" target="_blank">‘delta'</a> that requires processing.</p>
<p>The second component we are going to use is <a href="https://docs.snowflake.com/en/user-guide/tasks-intro.html" target="_blank">tasks</a>. It is a Snowflake managed data processing unit that will wake up on a defined interval (e.g., every 1-2 min), check if there is any data in the associated stream and if so, will run SQL to push it to the Raw Data Vault objects. Tasks could be arranged in a <a href="https://docs.snowflake.com/en/user-guide/tasks-intro.html#simple-tree-of-tasks" target="_blank">tree-like dependency graph</a>, executing child tasks the moment the predecessor finished its part.</p>
<p>Last but not least, following Data Vault 2.0 best practices for NRT data integration (to load data in parallel) we are going to use Snowflake's <a href="https://docs.snowflake.com/en/sql-reference/sql/insert-multi-table.html" target="_blank">multi-table insert (MTI)</a> inside tasks to populate multiple Raw Data Vault objects by a single DML command. (Alternatively you can create multiple streams &amp; tasks from the same table in stage in order to populate each data vault object by its own asynchronous flow.)</p>
<p>Next step, you assign tasks to one or many virtual warehouses. This means you always have enough <a href="https://docs.snowflake.com/en/user-guide/warehouses-overview.html#warehouse-size" target="_blank">compute power</a> (XS to 6XL) to deal with any size workload, whilst the <a href="https://docs.snowflake.com/en/user-guide/warehouses-multicluster.html#multi-cluster-warehouses" target="_blank">multi-cluster virtual warehouse</a> option will automatically scale-out and load balance all the tasks as you introduce more hubs, links and satellites to your vault.</p>
<p>Talking about tasks, Snowflake just introduced another fantastic capability - serverless tasks. This enables you to rely on compute resources managed by Snowflake instead of user-managed virtual warehouses. These compute resources are automatically resized and scaled up and down by Snowflake as required by each workload. This feature will be out of scope for this guide, but serverless compute model could reduce compute costs, in some cases significantly, allowing you to process more data, faster with even less management.</p>
<p>As your raw vault is updated, streams can then be used to propagate those changes to Business Vault objects (such as derived Sats, PITS, or Bridges, if needed) in the next layer. This setup can be repeated to move data through all the layers in small increments very quickly and efficiently. All the way until it is ready to be accessed by data consumers (if materialization of the data is required for performance).</p>
<p>Following this approach will result in a hands-off production data pipeline that feeds your Data Vault architecture.</p>


      </div></div></google-codelab-step><google-codelab-step label="Sample data &amp; staging area" duration="5" step="5" style="transform: translate3d(110%, 0px, 0px);"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">5. Sample data &amp; staging area</h2>
        <p>Every Snowflake account provides access to <a href="https://docs.snowflake.com/en/user-guide/sample-data.html" target="_blank">sample data sets</a>. You can find corresponding schemas in SNOWFLAKE_SAMPLE_DATA database in your object explorer. For this guide we are going to use a subset of objects from <a href="https://docs.snowflake.com/en/user-guide/sample-data-tpch.html" target="_blank">TPC-H</a> set, representing <strong>customers</strong> and their <strong>orders</strong>. We also going to take some reference data about <strong>nations</strong> and <strong>regions</strong>.</p>
<table>
<tbody><tr><td colspan="1" rowspan="1"><p>Dataset</p>
</td><td colspan="1" rowspan="1"><p>Description</p>
</td><td colspan="1" rowspan="1"><p>Source</p>
</td><td colspan="1" rowspan="1"><p>Load Scenario</p>
</td><td colspan="1" rowspan="1"><p>Mechanism</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Nation</p>
</td><td colspan="1" rowspan="1"><p>Static ref data</p>
</td><td colspan="1" rowspan="1"><p>snowflake.sample_data.tpch_sf10.nation</p>
</td><td colspan="1" rowspan="1"><p>one-off CTAS</p>
</td><td colspan="1" rowspan="1"><p>SQL</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Region</p>
</td><td colspan="1" rowspan="1"><p>Static ref data</p>
</td><td colspan="1" rowspan="1"><p>snowflake.sample_data.tpch_sf10.region</p>
</td><td colspan="1" rowspan="1"><p>one-off CTAS</p>
</td><td colspan="1" rowspan="1"><p>SQL</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Customer</p>
</td><td colspan="1" rowspan="1"><p>Customer data</p>
</td><td colspan="1" rowspan="1"><p>snowflake.sample_data.tpch_sf10.customer</p>
</td><td colspan="1" rowspan="1"><p>incremental JSON files</p>
</td><td colspan="1" rowspan="1"><p>Snowpipe</p>
</td></tr>
<tr><td colspan="1" rowspan="1"><p>Orders</p>
</td><td colspan="1" rowspan="1"><p>Orders data</p>
</td><td colspan="1" rowspan="1"><p>snowflake.sample_data.tpch_sf10.orders</p>
</td><td colspan="1" rowspan="1"><p>incremental CSV files</p>
</td><td colspan="1" rowspan="1"><p>Snowpipe</p>
</td></tr>
</tbody></table>
<ol type="1">
<li>Let's start with the static reference data:</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pun">--------------------------------------------------------------------</span><span class="pln">
</span><span class="pun">--</span><span class="pln"> setting up staging area
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">

USE SCHEMA l00_stg</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE TABLE stg_nation 
AS 
SELECT src</span><span class="pun">.*</span><span class="pln">
     </span><span class="pun">,</span><span class="pln"> CURRENT_TIMESTAMP</span><span class="pun">()</span><span class="pln">          ldts 
     </span><span class="pun">,</span><span class="pln"> </span><span class="str">'Static Reference Data'</span><span class="pln">      rscr 
  FROM snowflake_sample_data</span><span class="pun">.</span><span class="pln">tpch_sf10</span><span class="pun">.</span><span class="pln">nation src</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE TABLE stg_region
AS 
SELECT src</span><span class="pun">.*</span><span class="pln">
     </span><span class="pun">,</span><span class="pln"> CURRENT_TIMESTAMP</span><span class="pun">()</span><span class="pln">          ldts 
     </span><span class="pun">,</span><span class="pln"> </span><span class="str">'Static Reference Data'</span><span class="pln">      rscr 
  FROM snowflake_sample_data</span><span class="pun">.</span><span class="pln">tpch_sf10</span><span class="pun">.</span><span class="pln">region src</span><span class="pun">;</span></code></pre>
<ol type="1" start="2">
<li>Next, let's create staging tables for our data loading. This syntax should be very familiar with anyone working with databases before. It is ANSI SQL compliant DDL, with probably one key exception - for stg_customer we are going to load the full payload of JSON into raw_json column. For this, Snowflake has a special data type <a href="https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html" target="_blank">VARIANT</a>.</li>
</ol>
<p>As we load data we also going to add some technical metadata, like load data timestamp, row number in a file.</p>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE TABLE stg_customer
</span><span class="pun">(</span><span class="pln">
  raw_json                VARIANT
</span><span class="pun">,</span><span class="pln"> filename                STRING   NOT NULL
</span><span class="pun">,</span><span class="pln"> file_row_seq            NUMBER   NOT NULL
</span><span class="pun">,</span><span class="pln"> ldts                    STRING   NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                    STRING   NOT NULL
</span><span class="pun">);</span><span class="pln">

CREATE OR REPLACE TABLE stg_orders
</span><span class="pun">(</span><span class="pln">
  o_orderkey              NUMBER
</span><span class="pun">,</span><span class="pln"> o_custkey               NUMBER  
</span><span class="pun">,</span><span class="pln"> o_orderstatus           STRING
</span><span class="pun">,</span><span class="pln"> o_totalprice            NUMBER  
</span><span class="pun">,</span><span class="pln"> o_orderdate             DATE
</span><span class="pun">,</span><span class="pln"> o_orderpriority         STRING
</span><span class="pun">,</span><span class="pln"> o_clerk                 STRING
</span><span class="pun">,</span><span class="pln"> o_shippriority          NUMBER
</span><span class="pun">,</span><span class="pln"> o_comment               STRING
</span><span class="pun">,</span><span class="pln"> filename                STRING   NOT NULL
</span><span class="pun">,</span><span class="pln"> file_row_seq            NUMBER   NOT NULL
</span><span class="pun">,</span><span class="pln"> ldts                    STRING   NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                    STRING   NOT NULL
</span><span class="pun">);</span></code></pre>
<ol type="1" start="3">
<li>Tables we just created are going to be used by Snowpipe to drip-feed the data as it is lands in the stage. In order to easily detect and incrementally process the new portion of data we are going to create <a href="https://docs.snowflake.com/en/user-guide/streams.html" target="_blank">streams</a> on these staging tables:</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE STREAM stg_customer_strm ON TABLE stg_customer</span><span class="pun">;</span><span class="pln">
CREATE OR REPLACE STREAM stg_orders_strm ON TABLE stg_orders</span><span class="pun">;</span></code></pre>
<ol type="1" start="4">
<li>Next we are going to produce some sample data. And for the sake of simplicity we are going to take a bit of a shortcut here. We are going to generate data by unloading subset of data from our TPCH sample dataset into files and then use Snowpipe to load it back into our Data Vault lab, simulating the streaming feed. Let's start by creating two stages for each data class type (orders, customers data). In real-life scenarios these could be internal or external stages as well as these feeds could be sourced via Kafka connector. The world is your oyster.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE STAGE customer_data FILE_FORMAT </span><span class="pun">=</span><span class="pln"> </span><span class="pun">(</span><span class="pln">TYPE </span><span class="pun">=</span><span class="pln"> JSON</span><span class="pun">);</span><span class="pln">
CREATE OR REPLACE STAGE orders_data   FILE_FORMAT </span><span class="pun">=</span><span class="pln"> </span><span class="pun">(</span><span class="pln">TYPE </span><span class="pun">=</span><span class="pln"> CSV</span><span class="pun">)</span><span class="pln"> </span><span class="pun">;</span></code></pre>
<ol type="1" start="5">
<li>Generate and unload sample data. There are couple of things going on. First, we are using <a href="https://docs.snowflake.com/en/sql-reference/functions/object_construct.html" target="_blank">object_construct</a> as a quick way to create a object/document from all columns and subset of rows for customer data and offload it into customer_data stage. Orders data would be extracted into compressed CSV files. There are many additonal options in <a href="https://docs.snowflake.com/en/sql-reference/sql/copy-into-location.html" target="_blank">COPY INTO stage</a> construct that would fit most requirements, but in this case we are using INCLUDE_QUERY_ID to make it easier to generate new incremental files as we are going to run these commands over and over again, without a need to deal with file overriding.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">COPY INTO </span><span class="lit">@customer_data</span><span class="pln"> 
FROM
</span><span class="pun">(</span><span class="pln">SELECT object_construct</span><span class="pun">(*)</span><span class="pln">
  FROM snowflake_sample_data</span><span class="pun">.</span><span class="pln">tpch_sf10</span><span class="pun">.</span><span class="pln">customer limit </span><span class="lit">10</span><span class="pln">
</span><span class="pun">)</span><span class="pln"> 
INCLUDE_QUERY_ID</span><span class="pun">=</span><span class="pln">TRUE</span><span class="pun">;</span><span class="pln">

COPY INTO </span><span class="lit">@orders_data</span><span class="pln"> 
FROM
</span><span class="pun">(</span><span class="pln">SELECT </span><span class="pun">*</span><span class="pln">
  FROM snowflake_sample_data</span><span class="pun">.</span><span class="pln">tpch_sf10</span><span class="pun">.</span><span class="pln">orders limit </span><span class="lit">1000</span><span class="pln">
</span><span class="pun">)</span><span class="pln"> 
INCLUDE_QUERY_ID</span><span class="pun">=</span><span class="pln">TRUE</span><span class="pun">;</span></code></pre>
<p>You can now run the following to validate that the data is now stored in files:</p>
<pre><code language="language-sql" class="language-sql"><span class="pln">list </span><span class="lit">@customer_data</span><span class="pun">;</span><span class="pln">
SELECT METADATA$FILENAME</span><span class="pun">,</span><span class="pln">$1 FROM </span><span class="lit">@customer_data</span><span class="pun">;</span><span class="pln"> </span></code></pre>
<p class="image-container"><img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/52e3637cf6af2e42.png"></p>
<ol type="1" start="6">
<li>Next, we are going to setup Snowpipe to load data from files in a stage into staging tables. In this guide, for better transparency we are going to trigger Snowpipe explicitly to scan for new files, but in real projects you will likely going to enable AUTO_INGEST, connecting it with your cloud storage events (like AWS SNS) and process new files automatically.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE PIPE stg_orders_pp 
AS 
COPY INTO stg_orders 
FROM
</span><span class="pun">(</span><span class="pln">
SELECT $1</span><span class="pun">,</span><span class="pln">$2</span><span class="pun">,</span><span class="pln">$3</span><span class="pun">,</span><span class="pln">$4</span><span class="pun">,</span><span class="pln">$5</span><span class="pun">,</span><span class="pln">$6</span><span class="pun">,</span><span class="pln">$7</span><span class="pun">,</span><span class="pln">$8</span><span class="pun">,</span><span class="pln">$9 
     </span><span class="pun">,</span><span class="pln"> metadata$filename
     </span><span class="pun">,</span><span class="pln"> metadata$file_row_number
     </span><span class="pun">,</span><span class="pln"> CURRENT_TIMESTAMP</span><span class="pun">()</span><span class="pln">
     </span><span class="pun">,</span><span class="pln"> </span><span class="str">'Orders System'</span><span class="pln">
  FROM </span><span class="lit">@orders_data</span><span class="pln">
</span><span class="pun">);</span><span class="pln">

CREATE OR REPLACE PIPE stg_customer_pp 
</span><span class="pun">--</span><span class="pln">AUTO_INGEST </span><span class="pun">=</span><span class="pln"> TRUE
</span><span class="pun">--</span><span class="pln">aws_sns_topic </span><span class="pun">=</span><span class="pln"> </span><span class="str">'arn:aws:sns:mybucketdetails'</span><span class="pln">
AS 
COPY INTO stg_customer
FROM 
</span><span class="pun">(</span><span class="pln">
SELECT $1
     </span><span class="pun">,</span><span class="pln"> metadata$filename
     </span><span class="pun">,</span><span class="pln"> metadata$file_row_number
     </span><span class="pun">,</span><span class="pln"> CURRENT_TIMESTAMP</span><span class="pun">()</span><span class="pln">
     </span><span class="pun">,</span><span class="pln"> </span><span class="str">'Customers System'</span><span class="pln">
  FROM </span><span class="lit">@customer_data</span><span class="pln">
</span><span class="pun">);</span><span class="pln">

ALTER PIPE stg_customer_pp REFRESH</span><span class="pun">;</span><span class="pln">

ALTER PIPE stg_orders_pp   REFRESH</span><span class="pun">;</span></code></pre>
<p>Once this done, you should be able to see data appearling in the target tables and the stream on these tables. As you would notice, number of rows in a stream is exactly the same as in the base table. This is because we didn't process/consumed the delta of that stream yet. Stay tuned!</p>
<pre><code language="language-sql" class="language-sql"><span class="pln">SELECT </span><span class="str">'stg_customer'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM stg_customer
UNION ALL
SELECT </span><span class="str">'stg_orders'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM stg_orders
UNION ALL
SELECT </span><span class="str">'stg_orders_strm'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM stg_orders_strm
UNION ALL
SELECT </span><span class="str">'stg_customer_strm'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM stg_customer_strm
</span><span class="pun">;</span></code></pre>
<p class="image-container"><img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/e071a93cb5e9ca4c.png"></p>
<ol type="1" start="7">
<li>Finally, now that we established the basics and new data is knocking at our door (stream), let's see how we can derive some of the business keys for the Data Vault entites we are going to model. In this example, we will model it as a view on top of the stream that should allow us to perform data parsing (raw_json -&gt; columns) and business_key, hash_diff derivation on the fly. Another thing to notice here is the use of SHA1_BINARY as hasing function. There are many articles on choosing between MD5/SHA1(2)/other hash functions, so we won't focus on this. For this lab, we are going to use fairly common SHA1 and its BINARY version from Snowflake arsenal of functions that use less bytes to encode value than STRING.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE VIEW stg_customer_strm_outbound AS 
SELECT src</span><span class="pun">.*</span><span class="pln">
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_CUSTKEY</span><span class="pun">::</span><span class="pln">NUMBER           c_custkey
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_NAME</span><span class="pun">::</span><span class="pln">STRING              c_name
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_ADDRESS</span><span class="pun">::</span><span class="pln">STRING           c_address
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_NATIONKEY</span><span class="pun">::</span><span class="pln">NUMBER         C_nationcode
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_PHONE</span><span class="pun">::</span><span class="pln">STRING             c_phone
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_ACCTBAL</span><span class="pun">::</span><span class="pln">NUMBER           c_acctbal
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_MKTSEGMENT</span><span class="pun">::</span><span class="pln">STRING        c_mktsegment
     </span><span class="pun">,</span><span class="pln"> raw_json</span><span class="pun">:</span><span class="pln">C_COMMENT</span><span class="pun">::</span><span class="pln">STRING           c_comment     
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">
</span><span class="pun">--</span><span class="pln"> derived business key
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">
     </span><span class="pun">,</span><span class="pln"> SHA1_BINARY</span><span class="pun">(</span><span class="pln">UPPER</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_custkey</span><span class="pun">)))</span><span class="pln">  sha1_hub_customer     
     </span><span class="pun">,</span><span class="pln"> SHA1_BINARY</span><span class="pun">(</span><span class="pln">UPPER</span><span class="pun">(</span><span class="pln">ARRAY_TO_STRING</span><span class="pun">(</span><span class="pln">ARRAY_CONSTRUCT</span><span class="pun">(</span><span class="pln"> 
                                              NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_name</span><span class="pun">)</span><span class="pln">       </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">
                                            </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_address</span><span class="pun">)</span><span class="pln">    </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">              
                                            </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_nationcode</span><span class="pun">)</span><span class="pln"> </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">                 
                                            </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_phone</span><span class="pun">)</span><span class="pln">      </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">            
                                            </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_acctbal</span><span class="pun">)</span><span class="pln">    </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">               
                                            </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_mktsegment</span><span class="pun">)</span><span class="pln"> </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">                 
                                            </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">c_comment</span><span class="pun">)</span><span class="pln">    </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">               
                                            </span><span class="pun">),</span><span class="pln"> </span><span class="str">'^'</span><span class="pun">)))</span><span class="pln">  AS customer_hash_diff
  FROM stg_customer_strm src
</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE VIEW stg_order_strm_outbound AS 
SELECT src</span><span class="pun">.*</span><span class="pln">
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">
</span><span class="pun">--</span><span class="pln"> derived business key
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">
     </span><span class="pun">,</span><span class="pln"> SHA1_BINARY</span><span class="pun">(</span><span class="pln">UPPER</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_orderkey</span><span class="pun">)))</span><span class="pln">             sha1_hub_order
     </span><span class="pun">,</span><span class="pln"> SHA1_BINARY</span><span class="pun">(</span><span class="pln">UPPER</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_custkey</span><span class="pun">)))</span><span class="pln">              sha1_hub_customer  
     </span><span class="pun">,</span><span class="pln"> SHA1_BINARY</span><span class="pun">(</span><span class="pln">UPPER</span><span class="pun">(</span><span class="pln">ARRAY_TO_STRING</span><span class="pun">(</span><span class="pln">ARRAY_CONSTRUCT</span><span class="pun">(</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_orderkey</span><span class="pun">)</span><span class="pln">       </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">
                                                        </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_custkey</span><span class="pun">)</span><span class="pln">        </span><span class="pun">,</span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">
                                                        </span><span class="pun">),</span><span class="pln"> </span><span class="str">'^'</span><span class="pun">)))</span><span class="pln">  AS sha1_lnk_customer_order             
     </span><span class="pun">,</span><span class="pln"> SHA1_BINARY</span><span class="pun">(</span><span class="pln">UPPER</span><span class="pun">(</span><span class="pln">ARRAY_TO_STRING</span><span class="pun">(</span><span class="pln">ARRAY_CONSTRUCT</span><span class="pun">(</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_orderstatus</span><span class="pun">)</span><span class="pln">    </span><span class="pun">,</span><span class="pln"> </span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">         
                                                        </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_totalprice</span><span class="pun">)</span><span class="pln">     </span><span class="pun">,</span><span class="pln"> </span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">        
                                                        </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_orderdate</span><span class="pun">)</span><span class="pln">      </span><span class="pun">,</span><span class="pln"> </span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">       
                                                        </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_orderpriority</span><span class="pun">)</span><span class="pln">  </span><span class="pun">,</span><span class="pln"> </span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">           
                                                        </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_clerk</span><span class="pun">)</span><span class="pln">          </span><span class="pun">,</span><span class="pln"> </span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">    
                                                        </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_shippriority</span><span class="pun">)</span><span class="pln">   </span><span class="pun">,</span><span class="pln"> </span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">          
                                                        </span><span class="pun">,</span><span class="pln"> NVL</span><span class="pun">(</span><span class="pln">TRIM</span><span class="pun">(</span><span class="pln">o_comment</span><span class="pun">)</span><span class="pln">        </span><span class="pun">,</span><span class="pln"> </span><span class="str">'-1'</span><span class="pun">)</span><span class="pln">      
                                                        </span><span class="pun">),</span><span class="pln"> </span><span class="str">'^'</span><span class="pun">)))</span><span class="pln">  AS order_hash_diff     
  FROM stg_orders_strm src
</span><span class="pun">;</span></code></pre>
<p>Finally let's query these views to validate the results: <img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/b49a54dbbf4df6a8.png"></p>
<p>Well done! We build our staging/inbound pipeline, ready to accomodate streaming data and derived business keys that we are going to use in our Raw Data Vault. Let's move on to the next step!</p>


      </div></div></google-codelab-step><google-codelab-step label="Build: Raw Data Vault" duration="10" step="6"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">6. Build: Raw Data Vault</h2>
        <p>In this section, we will start building structures and pipelines for <strong>Raw Data Vault</strong> area.</p>
<p>Here is the ER model of the objects we are going to deploy using the script below: <img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/5e11ca9c4d39f42a.png"></p>
<ol type="1">
<li>We'll start by deploying DDL for the HUBs, LINKs and SATellite tables. As you can imagine, this guide has no chance to go in the detail on data vault modelling process. This is something we usually highly recommend to establish by working with experts &amp; partners from Data Vault Alliance.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pun">--------------------------------------------------------------------</span><span class="pln">
</span><span class="pun">--</span><span class="pln"> setting up RDV
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">

USE SCHEMA l10_rdv</span><span class="pun">;</span><span class="pln">

</span><span class="pun">--</span><span class="pln"> hubs

CREATE OR REPLACE TABLE hub_customer 
</span><span class="pun">(</span><span class="pln"> 
  sha1_hub_customer       BINARY    NOT NULL   
</span><span class="pun">,</span><span class="pln"> c_custkey               NUMBER    NOT NULL                                                                                 
</span><span class="pun">,</span><span class="pln"> ldts                    TIMESTAMP NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                    STRING    NOT NULL
</span><span class="pun">,</span><span class="pln"> CONSTRAINT pk_hub_customer        PRIMARY KEY</span><span class="pun">(</span><span class="pln">sha1_hub_customer</span><span class="pun">)</span><span class="pln">
</span><span class="pun">);</span><span class="pln">                                     

CREATE OR REPLACE TABLE hub_order 
</span><span class="pun">(</span><span class="pln"> 
  sha1_hub_order          BINARY    NOT NULL   
</span><span class="pun">,</span><span class="pln"> o_orderkey              NUMBER    NOT NULL                                                                                 
</span><span class="pun">,</span><span class="pln"> ldts                    TIMESTAMP NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                    STRING    NOT NULL
</span><span class="pun">,</span><span class="pln"> CONSTRAINT pk_hub_order           PRIMARY KEY</span><span class="pun">(</span><span class="pln">sha1_hub_order</span><span class="pun">)</span><span class="pln">
</span><span class="pun">);</span><span class="pln">                                     

</span><span class="pun">--</span><span class="pln"> sats

CREATE OR REPLACE TABLE sat_customer 
</span><span class="pun">(</span><span class="pln"> 
  sha1_hub_customer      BINARY    NOT NULL   
</span><span class="pun">,</span><span class="pln"> ldts                   TIMESTAMP NOT NULL
</span><span class="pun">,</span><span class="pln"> c_name                 STRING
</span><span class="pun">,</span><span class="pln"> c_address              STRING
</span><span class="pun">,</span><span class="pln"> c_phone                STRING 
</span><span class="pun">,</span><span class="pln"> c_acctbal              NUMBER
</span><span class="pun">,</span><span class="pln"> c_mktsegment           STRING    
</span><span class="pun">,</span><span class="pln"> c_comment              STRING
</span><span class="pun">,</span><span class="pln"> nationcode             NUMBER
</span><span class="pun">,</span><span class="pln"> hash_diff              BINARY    NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                   STRING    NOT NULL  
</span><span class="pun">,</span><span class="pln"> CONSTRAINT pk_sat_customer       PRIMARY KEY</span><span class="pun">(</span><span class="pln">sha1_hub_customer</span><span class="pun">,</span><span class="pln"> ldts</span><span class="pun">)</span><span class="pln">
</span><span class="pun">,</span><span class="pln"> CONSTRAINT fk_sat_customer       FOREIGN KEY</span><span class="pun">(</span><span class="pln">sha1_hub_customer</span><span class="pun">)</span><span class="pln"> REFERENCES hub_customer
</span><span class="pun">);</span><span class="pln">                                     

CREATE OR REPLACE TABLE sat_order 
</span><span class="pun">(</span><span class="pln"> 
  sha1_hub_order         BINARY    NOT NULL   
</span><span class="pun">,</span><span class="pln"> ldts                   TIMESTAMP NOT NULL
</span><span class="pun">,</span><span class="pln"> o_orderstatus          STRING   
</span><span class="pun">,</span><span class="pln"> o_totalprice           NUMBER
</span><span class="pun">,</span><span class="pln"> o_orderdate            DATE
</span><span class="pun">,</span><span class="pln"> o_orderpriority        STRING
</span><span class="pun">,</span><span class="pln"> o_clerk                STRING    
</span><span class="pun">,</span><span class="pln"> o_shippriority         NUMBER
</span><span class="pun">,</span><span class="pln"> o_comment              STRING
</span><span class="pun">,</span><span class="pln"> hash_diff              BINARY    NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                   STRING    NOT NULL   
</span><span class="pun">,</span><span class="pln"> CONSTRAINT pk_sat_order PRIMARY KEY</span><span class="pun">(</span><span class="pln">sha1_hub_order</span><span class="pun">,</span><span class="pln"> ldts</span><span class="pun">)</span><span class="pln">
</span><span class="pun">,</span><span class="pln"> CONSTRAINT fk_sat_order FOREIGN KEY</span><span class="pun">(</span><span class="pln">sha1_hub_order</span><span class="pun">)</span><span class="pln"> REFERENCES hub_order
</span><span class="pun">);</span><span class="pln">   

</span><span class="pun">--</span><span class="pln"> links

CREATE OR REPLACE TABLE lnk_customer_order
</span><span class="pun">(</span><span class="pln">
  sha1_lnk_customer_order BINARY     NOT NULL   
</span><span class="pun">,</span><span class="pln"> sha1_hub_customer       BINARY 
</span><span class="pun">,</span><span class="pln"> sha1_hub_order          BINARY 
</span><span class="pun">,</span><span class="pln"> ldts                    TIMESTAMP  NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                    STRING     NOT NULL  
</span><span class="pun">,</span><span class="pln"> CONSTRAINT pk_lnk_customer_order  PRIMARY KEY</span><span class="pun">(</span><span class="pln">sha1_lnk_customer_order</span><span class="pun">)</span><span class="pln">
</span><span class="pun">,</span><span class="pln"> CONSTRAINT fk1_lnk_customer_order FOREIGN KEY</span><span class="pun">(</span><span class="pln">sha1_hub_customer</span><span class="pun">)</span><span class="pln"> REFERENCES hub_customer
</span><span class="pun">,</span><span class="pln"> CONSTRAINT fk2_lnk_customer_order FOREIGN KEY</span><span class="pun">(</span><span class="pln">sha1_hub_order</span><span class="pun">)</span><span class="pln">    REFERENCES hub_order
</span><span class="pun">);</span><span class="pln">

</span><span class="pun">--</span><span class="pln"> </span><span class="kwd">ref</span><span class="pln"> data

CREATE OR REPLACE TABLE ref_region
</span><span class="pun">(</span><span class="pln"> 
  regioncode            NUMBER 
</span><span class="pun">,</span><span class="pln"> ldts                  TIMESTAMP
</span><span class="pun">,</span><span class="pln"> rscr                  STRING NOT NULL
</span><span class="pun">,</span><span class="pln"> r_name                STRING
</span><span class="pun">,</span><span class="pln"> r_comment             STRING
</span><span class="pun">,</span><span class="pln"> CONSTRAINT PK_REF_REGION PRIMARY KEY </span><span class="pun">(</span><span class="pln">REGIONCODE</span><span class="pun">)</span><span class="pln">                                                                             
</span><span class="pun">)</span><span class="pln">
AS 
SELECT r_regionkey
     </span><span class="pun">,</span><span class="pln"> ldts
     </span><span class="pun">,</span><span class="pln"> rscr
     </span><span class="pun">,</span><span class="pln"> r_name
     </span><span class="pun">,</span><span class="pln"> r_comment
  FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_region</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE TABLE ref_nation 
</span><span class="pun">(</span><span class="pln"> 
  nationcode            NUMBER 
</span><span class="pun">,</span><span class="pln"> regioncode            NUMBER 
</span><span class="pun">,</span><span class="pln"> ldts                  TIMESTAMP
</span><span class="pun">,</span><span class="pln"> rscr                  STRING NOT NULL
</span><span class="pun">,</span><span class="pln"> n_name                STRING
</span><span class="pun">,</span><span class="pln"> n_comment             STRING
</span><span class="pun">,</span><span class="pln"> CONSTRAINT pk_ref_nation PRIMARY KEY </span><span class="pun">(</span><span class="pln">nationcode</span><span class="pun">)</span><span class="pln">                                                                             
</span><span class="pun">,</span><span class="pln"> CONSTRAINT fk_ref_region FOREIGN KEY </span><span class="pun">(</span><span class="pln">regioncode</span><span class="pun">)</span><span class="pln"> REFERENCES ref_region</span><span class="pun">(</span><span class="pln">regioncode</span><span class="pun">)</span><span class="pln">  
</span><span class="pun">)</span><span class="pln">
AS 
SELECT n_nationkey
     </span><span class="pun">,</span><span class="pln"> n_regionkey
     </span><span class="pun">,</span><span class="pln"> ldts
     </span><span class="pun">,</span><span class="pln"> rscr
     </span><span class="pun">,</span><span class="pln"> n_name
     </span><span class="pun">,</span><span class="pln"> n_comment
  FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_nation</span><span class="pun">;</span><span class="pln">           </span></code></pre>
<ol type="1" start="2">
<li>Now we have source data waiting in our staging streams &amp; views, we have target RDV tables. Let's connect the dots. We are going to create tasks, one per each stream so whenever there is new records coming in a stream, that delta will be incrementally propagated to all dependent RDV models in one go. To achieve that, we are going to use multi-table insert functionality as described in design section before. As you can see, tasks can be set up to run on a pre-defined frequency (every 1 minute in our example) and use dedicated virtual warehouse as a compute power (in our guide we are going to use same warehouse for all tasks, thou this could be as granular as needed). Also, before waking up a compute resource, tasks are going to check that there is data in a corresponding stream to process. Again, you are paying only for the compute when you actually use it.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE TASK customer_strm_tsk
  WAREHOUSE </span><span class="pun">=</span><span class="pln"> dv_rdv_wh
  SCHEDULE </span><span class="pun">=</span><span class="pln"> </span><span class="str">'1 minute'</span><span class="pln">
WHEN
  SYSTEM$STREAM_HAS_DATA</span><span class="pun">(</span><span class="str">'L00_STG.STG_CUSTOMER_STRM'</span><span class="pun">)</span><span class="pln">
AS 
INSERT ALL
WHEN </span><span class="pun">(</span><span class="pln">SELECT COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM hub_customer tgt WHERE tgt</span><span class="pun">.</span><span class="pln">sha1_hub_customer </span><span class="pun">=</span><span class="pln"> src_sha1_hub_customer</span><span class="pun">)</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="lit">0</span><span class="pln">
THEN INTO hub_customer  
</span><span class="pun">(</span><span class="pln"> sha1_hub_customer
</span><span class="pun">,</span><span class="pln"> c_custkey
</span><span class="pun">,</span><span class="pln"> ldts
</span><span class="pun">,</span><span class="pln"> rscr
</span><span class="pun">)</span><span class="pln">  
VALUES 
</span><span class="pun">(</span><span class="pln"> src_sha1_hub_customer
</span><span class="pun">,</span><span class="pln"> src_c_custkey
</span><span class="pun">,</span><span class="pln"> src_ldts
</span><span class="pun">,</span><span class="pln"> src_rscr
</span><span class="pun">)</span><span class="pln">  
WHEN </span><span class="pun">(</span><span class="pln">SELECT COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM sat_customer tgt WHERE tgt</span><span class="pun">.</span><span class="pln">sha1_hub_customer </span><span class="pun">=</span><span class="pln"> src_sha1_hub_customer AND tgt</span><span class="pun">.</span><span class="pln">hash_diff </span><span class="pun">=</span><span class="pln"> src_customer_hash_diff</span><span class="pun">)</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="lit">0</span><span class="pln">
THEN INTO sat_customer  
</span><span class="pun">(</span><span class="pln">
  sha1_hub_customer  
</span><span class="pun">,</span><span class="pln"> ldts              
</span><span class="pun">,</span><span class="pln"> c_name            
</span><span class="pun">,</span><span class="pln"> c_address         
</span><span class="pun">,</span><span class="pln"> c_phone           
</span><span class="pun">,</span><span class="pln"> c_acctbal         
</span><span class="pun">,</span><span class="pln"> c_mktsegment      
</span><span class="pun">,</span><span class="pln"> c_comment         
</span><span class="pun">,</span><span class="pln"> nationcode        
</span><span class="pun">,</span><span class="pln"> hash_diff         
</span><span class="pun">,</span><span class="pln"> rscr              
</span><span class="pun">)</span><span class="pln">  
VALUES 
</span><span class="pun">(</span><span class="pln">
  src_sha1_hub_customer  
</span><span class="pun">,</span><span class="pln"> src_ldts              
</span><span class="pun">,</span><span class="pln"> src_c_name            
</span><span class="pun">,</span><span class="pln"> src_c_address         
</span><span class="pun">,</span><span class="pln"> src_c_phone           
</span><span class="pun">,</span><span class="pln"> src_c_acctbal         
</span><span class="pun">,</span><span class="pln"> src_c_mktsegment      
</span><span class="pun">,</span><span class="pln"> src_c_comment         
</span><span class="pun">,</span><span class="pln"> src_nationcode        
</span><span class="pun">,</span><span class="pln"> src_customer_hash_diff         
</span><span class="pun">,</span><span class="pln"> src_rscr              
</span><span class="pun">)</span><span class="pln">
SELECT sha1_hub_customer   src_sha1_hub_customer
     </span><span class="pun">,</span><span class="pln"> c_custkey           src_c_custkey
     </span><span class="pun">,</span><span class="pln"> c_name              src_c_name
     </span><span class="pun">,</span><span class="pln"> c_address           src_c_address
     </span><span class="pun">,</span><span class="pln"> c_nationcode        src_nationcode
     </span><span class="pun">,</span><span class="pln"> c_phone             src_c_phone
     </span><span class="pun">,</span><span class="pln"> c_acctbal           src_c_acctbal
     </span><span class="pun">,</span><span class="pln"> c_mktsegment        src_c_mktsegment
     </span><span class="pun">,</span><span class="pln"> c_comment           src_c_comment    
     </span><span class="pun">,</span><span class="pln"> customer_hash_diff  src_customer_hash_diff
     </span><span class="pun">,</span><span class="pln"> ldts                src_ldts
     </span><span class="pun">,</span><span class="pln"> rscr                src_rscr
  FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_customer_strm_outbound src
</span><span class="pun">;</span><span class="pln">


CREATE OR REPLACE TASK order_strm_tsk
  WAREHOUSE </span><span class="pun">=</span><span class="pln"> dv_rdv_wh
  SCHEDULE </span><span class="pun">=</span><span class="pln"> </span><span class="str">'1 minute'</span><span class="pln">
WHEN
  SYSTEM$STREAM_HAS_DATA</span><span class="pun">(</span><span class="str">'L00_STG.STG_ORDERS_STRM'</span><span class="pun">)</span><span class="pln">
AS 
INSERT ALL
WHEN </span><span class="pun">(</span><span class="pln">SELECT COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM hub_order tgt WHERE tgt</span><span class="pun">.</span><span class="pln">sha1_hub_order </span><span class="pun">=</span><span class="pln"> src_sha1_hub_order</span><span class="pun">)</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="lit">0</span><span class="pln">
THEN INTO hub_order  
</span><span class="pun">(</span><span class="pln"> sha1_hub_order
</span><span class="pun">,</span><span class="pln"> o_orderkey
</span><span class="pun">,</span><span class="pln"> ldts
</span><span class="pun">,</span><span class="pln"> rscr
</span><span class="pun">)</span><span class="pln">  
VALUES 
</span><span class="pun">(</span><span class="pln"> src_sha1_hub_order
</span><span class="pun">,</span><span class="pln"> src_o_orderkey
</span><span class="pun">,</span><span class="pln"> src_ldts
</span><span class="pun">,</span><span class="pln"> src_rscr
</span><span class="pun">)</span><span class="pln">  
WHEN </span><span class="pun">(</span><span class="pln">SELECT COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM sat_order tgt WHERE tgt</span><span class="pun">.</span><span class="pln">sha1_hub_order </span><span class="pun">=</span><span class="pln"> src_sha1_hub_order AND tgt</span><span class="pun">.</span><span class="pln">hash_diff </span><span class="pun">=</span><span class="pln"> src_order_hash_diff</span><span class="pun">)</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="lit">0</span><span class="pln">
THEN INTO sat_order  
</span><span class="pun">(</span><span class="pln">
  sha1_hub_order  
</span><span class="pun">,</span><span class="pln"> ldts              
</span><span class="pun">,</span><span class="pln"> o_orderstatus  
</span><span class="pun">,</span><span class="pln"> o_totalprice   
</span><span class="pun">,</span><span class="pln"> o_orderdate    
</span><span class="pun">,</span><span class="pln"> o_orderpriority
</span><span class="pun">,</span><span class="pln"> o_clerk        
</span><span class="pun">,</span><span class="pln"> o_shippriority 
</span><span class="pun">,</span><span class="pln"> o_comment              
</span><span class="pun">,</span><span class="pln"> hash_diff         
</span><span class="pun">,</span><span class="pln"> rscr              
</span><span class="pun">)</span><span class="pln">  
VALUES 
</span><span class="pun">(</span><span class="pln">
  src_sha1_hub_order  
</span><span class="pun">,</span><span class="pln"> src_ldts              
</span><span class="pun">,</span><span class="pln"> src_o_orderstatus  
</span><span class="pun">,</span><span class="pln"> src_o_totalprice   
</span><span class="pun">,</span><span class="pln"> src_o_orderdate    
</span><span class="pun">,</span><span class="pln"> src_o_orderpriority
</span><span class="pun">,</span><span class="pln"> src_o_clerk        
</span><span class="pun">,</span><span class="pln"> src_o_shippriority 
</span><span class="pun">,</span><span class="pln"> src_o_comment      
</span><span class="pun">,</span><span class="pln"> src_order_hash_diff         
</span><span class="pun">,</span><span class="pln"> src_rscr              
</span><span class="pun">)</span><span class="pln">
WHEN </span><span class="pun">(</span><span class="pln">SELECT COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM lnk_customer_order tgt WHERE tgt</span><span class="pun">.</span><span class="pln">sha1_lnk_customer_order </span><span class="pun">=</span><span class="pln"> src_sha1_lnk_customer_order</span><span class="pun">)</span><span class="pln"> </span><span class="pun">=</span><span class="pln"> </span><span class="lit">0</span><span class="pln">
THEN INTO lnk_customer_order  
</span><span class="pun">(</span><span class="pln">
  sha1_lnk_customer_order  
</span><span class="pun">,</span><span class="pln"> sha1_hub_customer              
</span><span class="pun">,</span><span class="pln"> sha1_hub_order  
</span><span class="pun">,</span><span class="pln"> ldts
</span><span class="pun">,</span><span class="pln"> rscr              
</span><span class="pun">)</span><span class="pln">  
VALUES 
</span><span class="pun">(</span><span class="pln">
  src_sha1_lnk_customer_order
</span><span class="pun">,</span><span class="pln"> src_sha1_hub_customer
</span><span class="pun">,</span><span class="pln"> src_sha1_hub_order  
</span><span class="pun">,</span><span class="pln"> src_ldts              
</span><span class="pun">,</span><span class="pln"> src_rscr              
</span><span class="pun">)</span><span class="pln">
SELECT sha1_hub_order          src_sha1_hub_order
     </span><span class="pun">,</span><span class="pln"> sha1_lnk_customer_order src_sha1_lnk_customer_order
     </span><span class="pun">,</span><span class="pln"> sha1_hub_customer       src_sha1_hub_customer
     </span><span class="pun">,</span><span class="pln"> o_orderkey              src_o_orderkey
     </span><span class="pun">,</span><span class="pln"> o_orderstatus           src_o_orderstatus  
     </span><span class="pun">,</span><span class="pln"> o_totalprice            src_o_totalprice   
     </span><span class="pun">,</span><span class="pln"> o_orderdate             src_o_orderdate    
     </span><span class="pun">,</span><span class="pln"> o_orderpriority         src_o_orderpriority
     </span><span class="pun">,</span><span class="pln"> o_clerk                 src_o_clerk        
     </span><span class="pun">,</span><span class="pln"> o_shippriority          src_o_shippriority 
     </span><span class="pun">,</span><span class="pln"> o_comment               src_o_comment      
     </span><span class="pun">,</span><span class="pln"> order_hash_diff         src_order_hash_diff
     </span><span class="pun">,</span><span class="pln"> ldts                    src_ldts
     </span><span class="pun">,</span><span class="pln"> rscr                    src_rscr
  FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_order_strm_outbound src</span><span class="pun">;</span><span class="pln">    

ALTER TASK customer_strm_tsk RESUME</span><span class="pun">;</span><span class="pln">  
ALTER TASK order_strm_tsk    RESUME</span><span class="pun">;</span><span class="pln">  
</span></code></pre>
<ol type="1" start="2">
<li>Once tasks are created and RESUMED (by default, they are initially suspended) let's have a look on the task execution history to see how the process will start.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">SELECT </span><span class="pun">*</span><span class="pln">
  FROM table</span><span class="pun">(</span><span class="pln">information_schema</span><span class="pun">.</span><span class="pln">task_history</span><span class="pun">())</span><span class="pln">
  ORDER BY scheduled_time DESC</span><span class="pun">;</span></code></pre>
<p>Notice how after successfull execution, next two tasks run were automatically SKIPPED as there were nothing in the stream and there nothing to do.</p>
<p class="image-container"><img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/4be750e52e75778d.png"></p>
<ol type="1" start="3">
<li>We can also check content and stats of the objects involved. Please notice that views on streams in our staging area are no longer returning any rows. This is because that delta of changes was consumed by a successfully completed DML transaction (in our case, embedded in tasks). This way you don't need to spend any time implementing incremental detection/processing logic on the application side.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">SELECT </span><span class="str">'hub_customer'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM hub_customer
UNION ALL
SELECT </span><span class="str">'hub_order'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM hub_order
UNION ALL
SELECT </span><span class="str">'sat_customer'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM sat_customer
UNION ALL
SELECT </span><span class="str">'sat_order'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM sat_order
UNION ALL
SELECT </span><span class="str">'lnk_customer_order'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM lnk_customer_order
UNION ALL
SELECT </span><span class="str">'l00_stg.stg_customer_strm_outbound'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_customer_strm_outbound
UNION ALL
SELECT </span><span class="str">'l00_stg.stg_order_strm_outbound'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_order_strm_outbound</span><span class="pun">;</span></code></pre>
<p class="image-container"><img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/5672cc4991043787.png"></p>
<p>Great. We now have data in our <strong>Raw Data Vault</strong> core structures. Let's move on and talk about the concept of virtualization for building your near-real time Data Vault solution.</p>


      </div></div></google-codelab-step><google-codelab-step label="Views for Agile Reporting" duration="10" step="7" style="transform: translate3d(110%, 0px, 0px);"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">7. Views for Agile Reporting</h2>
        <p>One of the great benefits of having the compute power from Snowflake is that now it is totally possible to have most of your business vault and information marts in a Data Vault architecture be built exclusively from views. There are numerous customers using this approach in production today. There is no longer a need to have the argument that there are "too many joins" or that the response won't be fast enough. The elasticity of the Snowflake virtual warehouses combined with our dynamic optimization engine have solved that problem. (For more details, see this <a href="https://www.snowflake.com/blog/tips-for-optimizing-the-data-vault-architecture-on-snowflake-part-3/" target="_blank">post</a>)</p>
<p>If you really want to deliver data to the business users and data scientists in NRT, in our opinion using views is the only option. Once you have the streaming loads built to feed your Data Vault, the fastest way to make that data visible downstream will be views. Using views allows you to deliver the data faster by eliminating any latency that would be incurred by having additional ELT processes between the Data Vault and the data consumers downstream.</p>
<p>All the business logic, alignment, and formatting of the data can be in the view code. That means fewer moving parts to debug, and reduces the storage needed as well.</p>
<p class="image-container"><img alt="dbt_project.yml" src="./Building a Real-Time Data Vault in Snowflake_files/edcd2073212da34b.png"></p>
<p>Looking at the diagram above you will see an example of how virtualization could fit in the architecture. Here, solid lines are representing physical tables and dotted lines - views. You incrementally ingest data into <strong>Raw Data Vault</strong> and all downstream transformations are applied as views. From a data consumer perspective when working with a virtualized information mart, the query always shows everything known by your data vault, right up to the point the query was submitted.</p>
<p>With Snowflake you have the ability to provide as much compute as required, on-demand,  without a risk of causing performance impact on any surrounding processes and pay only for what you use. This makes materialization of transformations in layers like <strong>Business Data Vault</strong> and <strong>Information delivery</strong> an option rather than a must-have. Instead of "optimizing upfront" you can now make this decision based on the usage pattern characteristics, such as frequency of use, type of queries, latency requirements, readiness of the requirements etc.</p>
<p>Many modern data engineering automation frameworks are already actively supporting virtualization of logic. Several tools offer a low-code or configuration-like ability to switch between materializing an object as a view or a physical table, automatically generating all required DDL &amp; DML. This could be applied on specific objects, layers or/and be environment specific. So even if you start with a view, you can easily refactor to use a table if user requirements evolve.</p>
<p>As said before, virtualization is not only a way to improve time-to-value and provide near real time access to the data, given the scalability and workload isolation of Snowflake, virtualization also is a design technique that could make your Data Vault excel: minimizing cost-of-change, accelerating the time-to-delivery and becoming an extremely agile, future proof solution for ever growing business needs.</p>


      </div></div></google-codelab-step><google-codelab-step label="Build: Business Data Vault" duration="10" step="8"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">8. Build: Business Data Vault</h2>
        <p>As a quick example of using views for transformations we just discussed, here is how enrichment of customer descriptive data could happen in Business Data Vault, connecting data received from source with some reference data.</p>
<ol type="1">
<li>Let's create a view that will perform these additional derivations on the fly. Assuming non-functional capabilities are satisflying our requirements, deploying (and re-deploying a new version) transformations in this way is super easy.</li>
</ol>
<pre><code language="language-SQL" class="language-SQL"><span class="pln">USE SCHEMA l20_bdv</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE VIEW sat_customer_bv
AS
SELECT rsc</span><span class="pun">.</span><span class="pln">sha1_hub_customer  
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">ldts                   
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">c_name                 
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">c_address              
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">c_phone                 
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">c_acctbal              
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">c_mktsegment               
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">c_comment              
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">nationcode             
     </span><span class="pun">,</span><span class="pln"> rsc</span><span class="pun">.</span><span class="pln">rscr 
     </span><span class="pun">--</span><span class="pln"> derived 
     </span><span class="pun">,</span><span class="pln"> rrn</span><span class="pun">.</span><span class="pln">n_name                    nation_name
     </span><span class="pun">,</span><span class="pln"> rrr</span><span class="pun">.</span><span class="pln">r_name                    region_name
  FROM l10_rdv</span><span class="pun">.</span><span class="pln">sat_customer          rsc
  LEFT OUTER JOIN l10_rdv</span><span class="pun">.</span><span class="pln">ref_nation rrn
    ON </span><span class="pun">(</span><span class="pln">rsc</span><span class="pun">.</span><span class="pln">nationcode </span><span class="pun">=</span><span class="pln"> rrn</span><span class="pun">.</span><span class="pln">nationcode</span><span class="pun">)</span><span class="pln">
  LEFT OUTER JOIN l10_rdv</span><span class="pun">.</span><span class="pln">ref_region rrr
    ON </span><span class="pun">(</span><span class="pln">rrn</span><span class="pun">.</span><span class="pln">regioncode </span><span class="pun">=</span><span class="pln"> rrr</span><span class="pun">.</span><span class="pln">regioncode</span><span class="pun">)</span><span class="pln">
</span><span class="pun">;</span></code></pre>
<ol type="1" start="2">
<li>Now,let's imagine we have a heavier transformation to perform that it would make more sense to materialize it as a table. It could be more data volume, could be more complex logic, PITs, bridges or even an object that will be used frequently and by many users. For this case, let's first build a new business satellite that for illustration purposes will be deriving additional classification/tiering for orders based on the conditional logic.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE TABLE sat_order_bv
</span><span class="pun">(</span><span class="pln"> 
  sha1_hub_order         BINARY    NOT NULL   
</span><span class="pun">,</span><span class="pln"> ldts                   TIMESTAMP NOT NULL
</span><span class="pun">,</span><span class="pln"> o_orderstatus          STRING   
</span><span class="pun">,</span><span class="pln"> o_totalprice           NUMBER
</span><span class="pun">,</span><span class="pln"> o_orderdate            DATE
</span><span class="pun">,</span><span class="pln"> o_orderpriority        STRING
</span><span class="pun">,</span><span class="pln"> o_clerk                STRING    
</span><span class="pun">,</span><span class="pln"> o_shippriority         NUMBER
</span><span class="pun">,</span><span class="pln"> o_comment              STRING  
</span><span class="pun">,</span><span class="pln"> hash_diff              BINARY    NOT NULL
</span><span class="pun">,</span><span class="pln"> rscr                   STRING    NOT NULL   
</span><span class="pun">--</span><span class="pln"> additional attributes
</span><span class="pun">,</span><span class="pln"> order_priority_bucket  STRING
</span><span class="pun">,</span><span class="pln"> CONSTRAINT pk_sat_order PRIMARY KEY</span><span class="pun">(</span><span class="pln">sha1_hub_order</span><span class="pun">,</span><span class="pln"> ldts</span><span class="pun">)</span><span class="pln">
</span><span class="pun">,</span><span class="pln"> CONSTRAINT fk_sat_order FOREIGN KEY</span><span class="pun">(</span><span class="pln">sha1_hub_order</span><span class="pun">)</span><span class="pln"> REFERENCES l10_rdv</span><span class="pun">.</span><span class="pln">hub_order
</span><span class="pun">)</span><span class="pln">
AS 
SELECT sha1_hub_order 
     </span><span class="pun">,</span><span class="pln"> ldts           
     </span><span class="pun">,</span><span class="pln"> o_orderstatus  
     </span><span class="pun">,</span><span class="pln"> o_totalprice   
     </span><span class="pun">,</span><span class="pln"> o_orderdate    
     </span><span class="pun">,</span><span class="pln"> o_orderpriority
     </span><span class="pun">,</span><span class="pln"> o_clerk        
     </span><span class="pun">,</span><span class="pln"> o_shippriority 
     </span><span class="pun">,</span><span class="pln"> o_comment      
     </span><span class="pun">,</span><span class="pln"> hash_diff      
     </span><span class="pun">,</span><span class="pln"> rscr 
     </span><span class="pun">--</span><span class="pln"> derived additional attributes
     </span><span class="pun">,</span><span class="pln"> CASE WHEN o_orderpriority IN </span><span class="pun">(</span><span class="str">'2-HIGH'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'1-URGENT'</span><span class="pun">)</span><span class="pln">             AND o_totalprice </span><span class="pun">&gt;=</span><span class="pln"> </span><span class="lit">200000</span><span class="pln"> THEN </span><span class="str">'Tier-1'</span><span class="pln">
            WHEN o_orderpriority IN </span><span class="pun">(</span><span class="str">'3-MEDIUM'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'2-HIGH'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'1-URGENT'</span><span class="pun">)</span><span class="pln"> AND o_totalprice BETWEEN </span><span class="lit">150000</span><span class="pln"> AND </span><span class="lit">200000</span><span class="pln"> THEN </span><span class="str">'Tier-2'</span><span class="pln">  
            ELSE </span><span class="str">'Tier-3'</span><span class="pln">
       </span><span class="kwd">END</span><span class="pln"> order_priority_bucket
  FROM l10_rdv</span><span class="pun">.</span><span class="pln">sat_order</span><span class="pun">;</span></code></pre>
<ol type="1" start="3">
<li>What we are going to do from processing/orchestration perspective is extending our order processing pipeline so that when the task populates a <strong>l10_rdv.sat_order</strong> this will generate a new stream of changes and these changes are going to be propagated by a dependent task to <strong>l20_bdv.sat_order_bv</strong>. This is super easy to do as tasks in Snowflake can be not only schedule-based but also start automatically once the parent task is completed.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">CREATE OR REPLACE STREAM l10_rdv</span><span class="pun">.</span><span class="pln">sat_order_strm ON TABLE l10_rdv</span><span class="pun">.</span><span class="pln">sat_order</span><span class="pun">;</span><span class="pln"> 

ALTER TASK l10_rdv</span><span class="pun">.</span><span class="pln">order_strm_tsk SUSPEND</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE TASK l10_rdv</span><span class="pun">.</span><span class="pln">hub_order_strm_sat_order_bv_tsk
  WAREHOUSE </span><span class="pun">=</span><span class="pln"> dv_rdv_wh
  AFTER l10_rdv</span><span class="pun">.</span><span class="pln">order_strm_tsk
AS 
INSERT INTO l20_bdv</span><span class="pun">.</span><span class="pln">sat_order_bv
SELECT   
  sha1_hub_order 
</span><span class="pun">,</span><span class="pln"> ldts           
</span><span class="pun">,</span><span class="pln"> o_orderstatus  
</span><span class="pun">,</span><span class="pln"> o_totalprice   
</span><span class="pun">,</span><span class="pln"> o_orderdate    
</span><span class="pun">,</span><span class="pln"> o_orderpriority
</span><span class="pun">,</span><span class="pln"> o_clerk        
</span><span class="pun">,</span><span class="pln"> o_shippriority 
</span><span class="pun">,</span><span class="pln"> o_comment      
</span><span class="pun">,</span><span class="pln"> hash_diff      
</span><span class="pun">,</span><span class="pln"> rscr 
</span><span class="pun">--</span><span class="pln"> derived additional attributes
</span><span class="pun">,</span><span class="pln"> CASE WHEN o_orderpriority IN </span><span class="pun">(</span><span class="str">'2-HIGH'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'1-URGENT'</span><span class="pun">)</span><span class="pln">             AND o_totalprice </span><span class="pun">&gt;=</span><span class="pln"> </span><span class="lit">200000</span><span class="pln"> THEN </span><span class="str">'Tier-1'</span><span class="pln">
       WHEN o_orderpriority IN </span><span class="pun">(</span><span class="str">'3-MEDIUM'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'2-HIGH'</span><span class="pun">,</span><span class="pln"> </span><span class="str">'1-URGENT'</span><span class="pun">)</span><span class="pln"> AND o_totalprice BETWEEN </span><span class="lit">150000</span><span class="pln"> AND </span><span class="lit">200000</span><span class="pln"> THEN </span><span class="str">'Tier-2'</span><span class="pln">  
       ELSE </span><span class="str">'Tier-3'</span><span class="pln">
  </span><span class="kwd">END</span><span class="pln"> order_priority_bucket
FROM sat_order_strm</span><span class="pun">;</span><span class="pln">

ALTER TASK l10_rdv</span><span class="pun">.</span><span class="pln">hub_order_strm_sat_order_bv_tsk RESUME</span><span class="pun">;</span><span class="pln">
ALTER TASK l10_rdv</span><span class="pun">.</span><span class="pln">order_strm_tsk RESUME</span><span class="pun">;</span></code></pre>
<ol type="1" start="4">
<li>Now, let's go back to our staging area to process another slice of data to test the task.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">USE SCHEMA l00_stg</span><span class="pun">;</span><span class="pln">

COPY INTO </span><span class="lit">@orders_data</span><span class="pln"> 
FROM
</span><span class="pun">(</span><span class="pln">SELECT </span><span class="pun">*</span><span class="pln">
  FROM snowflake_sample_data</span><span class="pun">.</span><span class="pln">tpch_sf10</span><span class="pun">.</span><span class="pln">orders limit </span><span class="lit">1000</span><span class="pln">
</span><span class="pun">)</span><span class="pln"> 
INCLUDE_QUERY_ID</span><span class="pun">=</span><span class="pln">TRUE</span><span class="pun">;</span><span class="pln">

ALTER PIPE stg_orders_pp   REFRESH</span><span class="pun">;</span></code></pre>
<ol type="1" start="5">
<li>Data is not automatically flowing through all the layers via asyncronous tasks. With the results you can validate:</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">SELECT </span><span class="str">'l00_stg.stg_orders'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_orders
UNION </span><span class="typ">ALl</span><span class="pln">
SELECT </span><span class="str">'l00_stg.stg_orders_strm'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM l00_stg</span><span class="pun">.</span><span class="pln">stg_orders_strm
UNION </span><span class="typ">ALl</span><span class="pln">
SELECT </span><span class="str">'l10_rdv.sat_order'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM l10_rdv</span><span class="pun">.</span><span class="pln">sat_order
UNION </span><span class="typ">ALl</span><span class="pln">
SELECT </span><span class="str">'l10_rdv.sat_order_strm'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM l10_rdv</span><span class="pun">.</span><span class="pln">sat_order_strm
UNION ALL
SELECT </span><span class="str">'l20_bdv.sat_order_bv'</span><span class="pun">,</span><span class="pln"> count</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM l20_bdv</span><span class="pun">.</span><span class="pln">sat_order_bv</span><span class="pun">;</span></code></pre>
<p class="image-container"><img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/2ad7414a1eec9c5e.png"></p>
<p>Great. Hope this example illustrated few ways of managing <strong>Business Data Vault</strong> objects in our pipeline. Let's finally move into the <strong>Information Delivery</strong> layer.</p>


      </div></div></google-codelab-step><google-codelab-step label="Build: Information Delivery" duration="10" step="9"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">9. Build: Information Delivery</h2>
        <p>When it comes to Information Delivery area we are not changing the meaning of data, but we may change format to simplify users to access and work with the data products/output interfaces. Different consumers may have different needs and preferences, some would prefer star/snowflake dimensional schemas, some would adhere to use flattened objects or even transform data into JSON/parquet objects.</p>
<ol type="1">
<li>First things we would like to add to simplify working with satellites is creating views that shows latest version for each key.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pun">--------------------------------------------------------------------</span><span class="pln">
</span><span class="pun">--</span><span class="pln"> RDV curr views
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">

USE SCHEMA l10_rdv</span><span class="pun">;</span><span class="pln">

CREATE VIEW sat_customer_curr_vw
AS
SELECT  </span><span class="pun">*</span><span class="pln">
FROM    sat_customer
QUALIFY LEAD</span><span class="pun">(</span><span class="pln">ldts</span><span class="pun">)</span><span class="pln"> OVER </span><span class="pun">(</span><span class="pln">PARTITION BY sha1_hub_customer ORDER BY ldts</span><span class="pun">)</span><span class="pln"> IS NULL</span><span class="pun">;</span><span class="pln">

CREATE OR REPLACE VIEW sat_order_curr_vw
AS
SELECT  </span><span class="pun">*</span><span class="pln">
FROM    sat_order
QUALIFY LEAD</span><span class="pun">(</span><span class="pln">ldts</span><span class="pun">)</span><span class="pln"> OVER </span><span class="pun">(</span><span class="pln">PARTITION BY sha1_hub_order ORDER BY ldts</span><span class="pun">)</span><span class="pln"> IS NULL</span><span class="pun">;</span><span class="pln">

</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">
</span><span class="pun">--</span><span class="pln"> BDV curr views
</span><span class="pun">--------------------------------------------------------------------</span><span class="pln">

USE SCHEMA l20_bdv</span><span class="pun">;</span><span class="pln">

CREATE VIEW sat_order_bv_curr_vw
AS
SELECT  </span><span class="pun">*</span><span class="pln">
FROM    sat_order_bv
QUALIFY LEAD</span><span class="pun">(</span><span class="pln">ldts</span><span class="pun">)</span><span class="pln"> OVER </span><span class="pun">(</span><span class="pln">PARTITION BY sha1_hub_order ORDER BY ldts</span><span class="pun">)</span><span class="pln"> IS NULL</span><span class="pun">;</span><span class="pln">

CREATE VIEW sat_customer_bv_curr_vw
AS
SELECT  </span><span class="pun">*</span><span class="pln">
FROM    sat_customer_bv
QUALIFY LEAD</span><span class="pun">(</span><span class="pln">ldts</span><span class="pun">)</span><span class="pln"> OVER </span><span class="pun">(</span><span class="pln">PARTITION BY sha1_hub_customer ORDER BY ldts</span><span class="pun">)</span><span class="pln"> IS NULL</span><span class="pun">;</span></code></pre>
<ol type="1" start="2">
<li>Let's create a simple dimensional structure. Again, we will keep it virtual(as views) to start with, but you already know that depending on access characteristics required any of these could be selectively materialized.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">USE SCHEMA l30_id</span><span class="pun">;</span><span class="pln">

</span><span class="pun">--</span><span class="pln"> DIM TYPE </span><span class="lit">1</span><span class="pln">
CREATE OR REPLACE VIEW dim1_customer 
AS 
SELECT hub</span><span class="pun">.</span><span class="pln">sha1_hub_customer                      AS dim_customer_key
     </span><span class="pun">,</span><span class="pln"> sat</span><span class="pun">.</span><span class="pln">ldts                                   AS effective_dts
     </span><span class="pun">,</span><span class="pln"> hub</span><span class="pun">.</span><span class="pln">c_custkey                              AS customer_id
     </span><span class="pun">,</span><span class="pln"> sat</span><span class="pun">.</span><span class="pln">rscr                                   AS record_source
     </span><span class="pun">,</span><span class="pln"> sat</span><span class="pun">.*</span><span class="pln">     
  FROM l10_rdv</span><span class="pun">.</span><span class="pln">hub_customer                       hub
     </span><span class="pun">,</span><span class="pln"> l20_bdv</span><span class="pun">.</span><span class="pln">sat_customer_bv_curr_vw            sat
 WHERE hub</span><span class="pun">.</span><span class="pln">sha1_hub_customer                      </span><span class="pun">=</span><span class="pln"> sat</span><span class="pun">.</span><span class="pln">sha1_hub_customer</span><span class="pun">;</span><span class="pln">

</span><span class="pun">--</span><span class="pln"> DIM TYPE </span><span class="lit">1</span><span class="pln">
CREATE OR REPLACE VIEW dim1_order
AS 
SELECT hub</span><span class="pun">.</span><span class="pln">sha1_hub_order                         AS dim_order_key
     </span><span class="pun">,</span><span class="pln"> sat</span><span class="pun">.</span><span class="pln">ldts                                   AS effective_dts
     </span><span class="pun">,</span><span class="pln"> hub</span><span class="pun">.</span><span class="pln">o_orderkey                             AS order_id
     </span><span class="pun">,</span><span class="pln"> sat</span><span class="pun">.</span><span class="pln">rscr                                   AS record_source
     </span><span class="pun">,</span><span class="pln"> sat</span><span class="pun">.*</span><span class="pln">
  FROM l10_rdv</span><span class="pun">.</span><span class="pln">hub_order                          hub
     </span><span class="pun">,</span><span class="pln"> l20_bdv</span><span class="pun">.</span><span class="pln">sat_order_bv_curr_vw               sat
 WHERE hub</span><span class="pun">.</span><span class="pln">sha1_hub_order                         </span><span class="pun">=</span><span class="pln"> sat</span><span class="pun">.</span><span class="pln">sha1_hub_order</span><span class="pun">;</span><span class="pln">

</span><span class="pun">--</span><span class="pln"> FACT table

CREATE OR REPLACE VIEW fct_customer_order
AS
SELECT lnk</span><span class="pun">.</span><span class="pln">ldts                                   AS effective_dts     
     </span><span class="pun">,</span><span class="pln"> lnk</span><span class="pun">.</span><span class="pln">rscr                                   AS record_source
     </span><span class="pun">,</span><span class="pln"> lnk</span><span class="pun">.</span><span class="pln">sha1_hub_customer                      AS dim_customer_key
     </span><span class="pun">,</span><span class="pln"> lnk</span><span class="pun">.</span><span class="pln">sha1_hub_order                         AS dim_order_key
</span><span class="pun">--</span><span class="pln"> </span><span class="kwd">this</span><span class="pln"> </span><span class="kwd">is</span><span class="pln"> a factless fact</span><span class="pun">,</span><span class="pln"> but here you can add any measures</span><span class="pun">,</span><span class="pln"> calculated </span><span class="kwd">or</span><span class="pln"> derived
  FROM l10_rdv</span><span class="pun">.</span><span class="pln">lnk_customer_order                 lnk</span><span class="pun">;</span><span class="pln">  </span></code></pre>
<ol type="1" start="3">
<li>All good so far? Now lets try to query <strong>fct_customer_order</strong> and at least in my case this view was not returning any rows. Why? If you remember, when we were unloading sample data, we took a subset of random orders and a subset of random customers. Which in my case didn't have any overlap, therefore doing the inner join with dim1_order was resulting in all rows being eliminated from the resultset. Thankfully we are using Data Vault and all need to do is go and load full customer dataset. Just think about it, there is no need to reprocess any links or fact tables simply because customer/reference feed was incomplete. I am sure for those of you who were using different architectures for data engineering and warehousing have painful experience when such situations occur. So, lets go and fix it:</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">USE SCHEMA l00_stg</span><span class="pun">;</span><span class="pln">

COPY INTO </span><span class="lit">@customer_data</span><span class="pln"> 
FROM
</span><span class="pun">(</span><span class="pln">SELECT object_construct</span><span class="pun">(*)</span><span class="pln">
  FROM snowflake_sample_data</span><span class="pun">.</span><span class="pln">tpch_sf10</span><span class="pun">.</span><span class="pln">customer
  </span><span class="pun">--</span><span class="pln"> removed LIMIT </span><span class="lit">10</span><span class="pln">
</span><span class="pun">)</span><span class="pln"> 
INCLUDE_QUERY_ID</span><span class="pun">=</span><span class="pln">TRUE</span><span class="pun">;</span><span class="pln">

ALTER PIPE stg_customer_pp   REFRESH</span><span class="pun">;</span></code></pre>
<p>All you need to do now is just wait a few seconds whilst our continious data pipeline will automatically propagate new customer data into Raw Data Vault. Quick check for the records count in customer dimension now shows that there are 1.5Mn records:</p>
<pre><code language="language-sql" class="language-sql"><span class="pln">USE SCHEMA l30_id</span><span class="pun">;</span><span class="pln">

SELECT COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln"> FROM dim1_customer</span><span class="pun">;</span><span class="pln">


COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln">
</span><span class="pun">------</span><span class="pln">
</span><span class="lit">1</span><span class="pun">,</span><span class="lit">500</span><span class="pun">,</span><span class="lit">000</span></code></pre>
<ol type="1" start="4">
<li>Finally lets wear user's hat and run a query to break down orders by nation,region and order_priority_bucket (all attributes we derived in <strong>Business Data Vault</strong>). As we are using Snowsight, why not quickly creating a chart from this result set to better understand the data. For this simply click on the ‘Chart' section on the bottom pane and put attributes/measures as it is snown on the screenshot below.</li>
</ol>
<pre><code language="language-sql" class="language-sql"><span class="pln">SELECT dc</span><span class="pun">.</span><span class="pln">nation_name
     </span><span class="pun">,</span><span class="pln"> dc</span><span class="pun">.</span><span class="pln">region_name
     </span><span class="pun">,</span><span class="pln"> </span><span class="kwd">do</span><span class="pun">.</span><span class="pln">order_priority_bucket
     </span><span class="pun">,</span><span class="pln"> COUNT</span><span class="pun">(</span><span class="lit">1</span><span class="pun">)</span><span class="pln">                                   cnt_orders
  FROM fct_customer_order                         fct
     </span><span class="pun">,</span><span class="pln"> dim1_customer                              dc
     </span><span class="pun">,</span><span class="pln"> dim1_order                                 </span><span class="kwd">do</span><span class="pln">
 WHERE fct</span><span class="pun">.</span><span class="pln">dim_customer_key                       </span><span class="pun">=</span><span class="pln"> dc</span><span class="pun">.</span><span class="pln">dim_customer_key
   AND fct</span><span class="pun">.</span><span class="pln">dim_order_key                          </span><span class="pun">=</span><span class="pln"> </span><span class="kwd">do</span><span class="pun">.</span><span class="pln">dim_order_key
GROUP BY </span><span class="lit">1</span><span class="pun">,</span><span class="lit">2</span><span class="pun">,</span><span class="lit">3</span><span class="pun">;</span></code></pre>
<p class="image-container"><img alt="staged data" src="./Building a Real-Time Data Vault in Snowflake_files/764aecfddb2d8369.png"></p>
<p>Voila! This concludes our journey for this guide. Hope you enjoyed it and lets summarise key points in the next section.</p>


      </div></div></google-codelab-step><google-codelab-step label="Conclusion" duration="1" step="10"><div class="instructions"><div class="inner"><h2 is-upgraded="" class="step-title">10. Conclusion</h2>
        <p>Simplicity of engineering, openness, scalable performance, enterprise-grade governance enabled by the core of the Snowflake platform are now allowing teams to focus on what matters most for the business and build truly agile, collaborative data environments. Teams can now connect data from all parts of the landscape, until there are no stones left unturned. They are even tapping into new datasets via live access to the Snowflake Data Marketplace. The Snowflake Data Cloud combined with a Data Vault 2.0 approach is allowing teams to democratize access to all their data assets at any scale. We can now easily derive more and more value through insights and intelligence, day after day, bringing businesses to the next level of being truly data-driven.</p>
<p>Delivering more usable data faster is no longer an option for today's business environment. Using the Snowflake platform, combined with the Data Vault 2.0 architecture it is now possible to build a world class analytics platform that delivers data for all users in near real-time.</p>
<h2 class="checklist" is-upgraded="">What we've covered</h2>
<ul class="checklist">
<li>unloading and loading back data using COPY and Snowpipe</li>
<li>engineering data pipelines using virtualization, streams and tasks</li>
<li>building multi-layer Data Vault environment on Snowflake:</li>
</ul>
<p class="image-container"><img alt="dbt_project.yml" src="./Building a Real-Time Data Vault in Snowflake_files/4d7304b1e9b552a9.png"></p>
<h2 is-upgraded="">Call to action</h2>
<ul>
<li>seeing is believing. Try it!</li>
<li>we made examples limited in size, but feel free to scale the data volumes and virtual warehouse size to see scalability in action</li>
<li>tap into numerous communities of practice for Data Engineering on Snowflake and Data Vault in particular</li>
<li>talk to us about modernizing your data landscape! Whether it is Data Vault or not you have on your mind, we have the top expertise and product to meet your demand</li>
<li>feedback is super welcome!</li>
<li>Enjoy your journey!</li>
</ul>


      </div></div></google-codelab-step></div><div id="controls"><div id="fabs"><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#" id="previous-step" title="Previous step" disappear="">Back</a><div class="spacer"></div><a href="https://quickstarts.snowflake.com/guide/vhol_data_vault/#" id="next-step" title="Next step">Next</a><a href="https://quickstarts.snowflake.com/" id="done" hidden="" title="Codelab complete">Done</a></div></div></div></google-codelab>

  <script src="./Building a Real-Time Data Vault in Snowflake_files/native-shim.js.descarga"></script>
  <script src="./Building a Real-Time Data Vault in Snowflake_files/custom-elements.min.js.descarga"></script>
  <script src="./Building a Real-Time Data Vault in Snowflake_files/prettify.js.descarga"></script>
  <script src="./Building a Real-Time Data Vault in Snowflake_files/codelab-elements.js.descarga"></script>
  <script src="./Building a Real-Time Data Vault in Snowflake_files/api.js.descarga"></script>
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
    heap.load("2025084205");
  </script>


</body></html>